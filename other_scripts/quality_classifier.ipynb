{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Photo Quality Classifier — Stage 1: Good / Bad / Unreadable\n\n**Goal:** Train a 3-class classifier for data sticker photos:\n- **Good** — clear, legible, usable\n- **Bad** — poor photo quality (blurry, dark, angled) — tech should retake\n- **Unreadable** — physical label is damaged/faded beyond recovery — retake won't help\n\n**Model:** MobileNetV2 with transfer learning (pre-trained on ImageNet).\nLightweight, fast inference, easy to deploy later.\n\n**Workflow:**\n1. Upload zipped `good/`, `bad/`, and `unreadable/` folders\n2. Data augmentation to increase training variety\n3. Train with frozen base → fine-tune top layers\n4. Evaluate with accuracy, precision, recall, F1, confusion matrix\n5. Export as `.h5` and TFLite\n\n---\n\n**Run in Google Colab with GPU enabled:**\nRuntime → Change runtime type → GPU"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Setup & Upload Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import zipfile\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import tensorflow as tf\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers\n",
    "from sklearn.metrics import classification_report, confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "print(f\"TensorFlow version: {tf.__version__}\")\n",
    "print(f\"GPU available: {len(tf.config.list_physical_devices('GPU')) > 0}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Upload your training data zip file.\n# Expected structure inside the zip:\n#   sorted/good/*.jpg\n#   sorted/bad/*.jpg\n#   sorted/unreadable/*.jpg\n\nfrom google.colab import files\n\nprint(\"Upload your training data zip (sorted.zip containing good/, bad/, and unreadable/ folders):\")\nuploaded = files.upload()\n\nzip_filename = list(uploaded.keys())[0]\nprint(f\"Uploaded: {zip_filename}\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Extract the zip\nwith zipfile.ZipFile(zip_filename, 'r') as z:\n    z.extractall('training_data')\n\n# Auto-detect the data directory structure\n# Supports: training_data/good & training_data/bad & training_data/unreadable\n#       or: training_data/sorted/good & training_data/sorted/bad & training_data/sorted/unreadable\nif os.path.isdir('training_data/sorted/good'):\n    DATA_DIR = 'training_data/sorted'\nelif os.path.isdir('training_data/good'):\n    DATA_DIR = 'training_data'\nelse:\n    # List what was extracted to help debug\n    for root, dirs, _files in os.walk('training_data'):\n        for d in dirs:\n            print(os.path.join(root, d))\n    raise FileNotFoundError(\"Could not find good/, bad/, unreadable/ folders. Check zip structure.\")\n\ngood_count = len(os.listdir(os.path.join(DATA_DIR, 'good')))\nbad_count = len(os.listdir(os.path.join(DATA_DIR, 'bad')))\nunreadable_count = len(os.listdir(os.path.join(DATA_DIR, 'unreadable')))\nprint(f\"Data directory: {DATA_DIR}\")\nprint(f\"Good photos:       {good_count}\")\nprint(f\"Bad photos:        {bad_count}\")\nprint(f\"Unreadable photos: {unreadable_count}\")\nprint(f\"Total:             {good_count + bad_count + unreadable_count}\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Data Loading & Augmentation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Configuration\n",
    "IMG_SIZE = 224        # MobileNetV2 default input size\n",
    "BATCH_SIZE = 32\n",
    "VALIDATION_SPLIT = 0.2\n",
    "SEED = 42"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Load training set (3 classes: bad=0, good=1, unreadable=2 — alphabetical)\ntrain_ds = keras.utils.image_dataset_from_directory(\n    DATA_DIR,\n    validation_split=VALIDATION_SPLIT,\n    subset='training',\n    seed=SEED,\n    image_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    label_mode='categorical',\n)\n\n# Load validation set\nval_ds = keras.utils.image_dataset_from_directory(\n    DATA_DIR,\n    validation_split=VALIDATION_SPLIT,\n    subset='validation',\n    seed=SEED,\n    image_size=(IMG_SIZE, IMG_SIZE),\n    batch_size=BATCH_SIZE,\n    label_mode='categorical',\n)\n\nclass_names = train_ds.class_names\nNUM_CLASSES = len(class_names)\nprint(f\"Classes ({NUM_CLASSES}): {class_names}\")\nprint(f\"Class mapping: \" + \", \".join(f\"{name}={i}\" for i, name in enumerate(class_names)))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Preview a batch of training images\nplt.figure(figsize=(12, 8))\nfor images, labels in train_ds.take(1):\n    for i in range(min(12, len(images))):\n        ax = plt.subplot(3, 4, i + 1)\n        plt.imshow(images[i].numpy().astype('uint8'))\n        label_idx = np.argmax(labels[i].numpy())\n        plt.title(class_names[label_idx])\n        plt.axis('off')\nplt.suptitle('Sample Training Images', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation layer (applied only during training)\n",
    "data_augmentation = keras.Sequential([\n",
    "    layers.RandomFlip('horizontal'),\n",
    "    layers.RandomRotation(0.15),\n",
    "    layers.RandomZoom(0.1),\n",
    "    layers.RandomBrightness(0.2),\n",
    "    layers.RandomContrast(0.2),\n",
    "], name='data_augmentation')\n",
    "\n",
    "# Preview augmented images\n",
    "plt.figure(figsize=(12, 4))\n",
    "for images, _ in train_ds.take(1):\n",
    "    original = images[0]\n",
    "    for i in range(6):\n",
    "        ax = plt.subplot(1, 6, i + 1)\n",
    "        augmented = data_augmentation(tf.expand_dims(original, 0))\n",
    "        plt.imshow(augmented[0].numpy().astype('uint8'))\n",
    "        plt.axis('off')\n",
    "        plt.title('aug' if i > 0 else 'original')\n",
    "plt.suptitle('Augmentation Preview', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Performance optimization: prefetch data\n",
    "AUTOTUNE = tf.data.AUTOTUNE\n",
    "train_ds = train_ds.cache().shuffle(1000).prefetch(buffer_size=AUTOTUNE)\n",
    "val_ds = val_ds.cache().prefetch(buffer_size=AUTOTUNE)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Build Model (MobileNetV2 + Transfer Learning)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load pre-trained MobileNetV2 (without top classification layer)\n",
    "base_model = keras.applications.MobileNetV2(\n",
    "    input_shape=(IMG_SIZE, IMG_SIZE, 3),\n",
    "    include_top=False,\n",
    "    weights='imagenet',\n",
    ")\n",
    "\n",
    "# Freeze the base model — we'll only train the new head initially\n",
    "base_model.trainable = False\n",
    "\n",
    "print(f\"Base model layers: {len(base_model.layers)}\")\n",
    "print(f\"Base model params: {base_model.count_params():,}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Build the full model\ninputs = keras.Input(shape=(IMG_SIZE, IMG_SIZE, 3))\n\n# Augmentation (only active during training)\nx = data_augmentation(inputs)\n\n# MobileNetV2 preprocessing (scales pixels to [-1, 1])\nx = keras.applications.mobilenet_v2.preprocess_input(x)\n\n# Base model (frozen)\nx = base_model(x, training=False)\n\n# Classification head — 3 classes with softmax\nx = layers.GlobalAveragePooling2D()(x)\nx = layers.Dropout(0.3)(x)\nx = layers.Dense(128, activation='relu')(x)\nx = layers.Dropout(0.2)(x)\noutputs = layers.Dense(NUM_CLASSES, activation='softmax')(x)\n\nmodel = keras.Model(inputs, outputs, name='quality_classifier')\nmodel.summary()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Train — Phase 1: Frozen Base"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "model.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-3),\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n)\n\nearly_stop = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=3,\n    restore_best_weights=True,\n)\n\nprint(\"Phase 1: Training classification head (base frozen)...\")\nhistory1 = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=10,\n    callbacks=[early_stop],\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Train — Phase 2: Fine-Tune Top Layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Unfreeze the top ~30% of the base model for fine-tuning\nbase_model.trainable = True\nfine_tune_from = 100  # MobileNetV2 has 154 layers — freeze first 100\n\nfor layer in base_model.layers[:fine_tune_from]:\n    layer.trainable = False\n\ntrainable_count = sum(1 for l in base_model.layers if l.trainable)\nprint(f\"Fine-tuning {trainable_count} of {len(base_model.layers)} base layers\")\n\n# Use a lower learning rate for fine-tuning to avoid catastrophic forgetting\nmodel.compile(\n    optimizer=keras.optimizers.Adam(learning_rate=1e-4),\n    loss='categorical_crossentropy',\n    metrics=['accuracy'],\n)\n\nearly_stop_ft = keras.callbacks.EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    restore_best_weights=True,\n)\n\nprint(\"Phase 2: Fine-tuning top layers...\")\nhistory2 = model.fit(\n    train_ds,\n    validation_data=val_ds,\n    epochs=20,\n    callbacks=[early_stop_ft],\n)"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Training History"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Combine training histories from both phases\n",
    "def combine_histories(h1, h2):\n",
    "    combined = {}\n",
    "    for key in h1.history:\n",
    "        combined[key] = h1.history[key] + h2.history[key]\n",
    "    return combined\n",
    "\n",
    "history = combine_histories(history1, history2)\n",
    "phase1_epochs = len(history1.history['loss'])\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2, figsize=(14, 5))\n",
    "\n",
    "# Accuracy\n",
    "ax1.plot(history['accuracy'], label='Train')\n",
    "ax1.plot(history['val_accuracy'], label='Validation')\n",
    "ax1.axvline(x=phase1_epochs - 0.5, color='gray', linestyle='--', alpha=0.5, label='Fine-tune start')\n",
    "ax1.set_title('Model Accuracy')\n",
    "ax1.set_xlabel('Epoch')\n",
    "ax1.set_ylabel('Accuracy')\n",
    "ax1.legend()\n",
    "ax1.grid(True, alpha=0.3)\n",
    "\n",
    "# Loss\n",
    "ax2.plot(history['loss'], label='Train')\n",
    "ax2.plot(history['val_loss'], label='Validation')\n",
    "ax2.axvline(x=phase1_epochs - 0.5, color='gray', linestyle='--', alpha=0.5, label='Fine-tune start')\n",
    "ax2.set_title('Model Loss')\n",
    "ax2.set_xlabel('Epoch')\n",
    "ax2.set_ylabel('Loss')\n",
    "ax2.legend()\n",
    "ax2.grid(True, alpha=0.3)\n",
    "\n",
    "plt.suptitle('Training History', fontsize=14)\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Print final metrics\n",
    "print(f\"\\nFinal training accuracy:   {history['accuracy'][-1]:.4f}\")\n",
    "print(f\"Final validation accuracy: {history['val_accuracy'][-1]:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 7. Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Collect all validation predictions\ny_true = []\ny_pred_list = []\n\nfor images, labels in val_ds:\n    preds = model.predict(images, verbose=0)\n    y_true.extend(np.argmax(labels.numpy(), axis=1))\n    y_pred_list.extend(np.argmax(preds, axis=1))\n\ny_true = np.array(y_true)\ny_pred = np.array(y_pred_list)\n\n# Classification report\nprint(\"Classification Report:\")\nprint(\"=\" * 55)\nprint(classification_report(y_true, y_pred, target_names=class_names))"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Confusion matrix\ncm = confusion_matrix(y_true, y_pred)\ndisp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=class_names)\n\nfig, ax = plt.subplots(figsize=(7, 7))\ndisp.plot(ax=ax, cmap='Blues', values_format='d')\nplt.title('Confusion Matrix — Validation Set')\nplt.tight_layout()\nplt.show()\n\n# Summary stats\ntotal = len(y_true)\ncorrect = (y_pred == y_true).sum()\nprint(f\"\\nValidation: {correct}/{total} correct ({correct/total*100:.1f}%)\")"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# Sample predictions on validation images\nplt.figure(figsize=(16, 8))\nshown = 0\nfor images, labels in val_ds:\n    preds = model.predict(images, verbose=0)\n    for i in range(len(images)):\n        if shown >= 16:\n            break\n        ax = plt.subplot(4, 4, shown + 1)\n        plt.imshow(images[i].numpy().astype('uint8'))\n        true_idx = np.argmax(labels[i].numpy())\n        pred_idx = np.argmax(preds[i])\n        confidence = preds[i][pred_idx]\n        true_label = class_names[true_idx]\n        pred_label = class_names[pred_idx]\n        color = 'green' if true_idx == pred_idx else 'red'\n        plt.title(f'T:{true_label} P:{pred_label}\\n{confidence:.0%}', color=color, fontsize=9)\n        plt.axis('off')\n        shown += 1\n    if shown >= 16:\n        break\n\nplt.suptitle('Sample Predictions (green=correct, red=wrong)', fontsize=14)\nplt.tight_layout()\nplt.show()"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 8. Export Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Save as .h5 (Keras format)\n",
    "h5_path = 'quality_classifier.h5'\n",
    "model.save(h5_path)\n",
    "h5_size = os.path.getsize(h5_path) / (1024 * 1024)\n",
    "print(f\"Saved Keras model: {h5_path} ({h5_size:.1f} MB)\")\n",
    "\n",
    "# Save as SavedModel format (for TFLite conversion)\n",
    "saved_model_dir = 'quality_classifier_saved_model'\n",
    "model.save(saved_model_dir)\n",
    "print(f\"Saved TF SavedModel: {saved_model_dir}/\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to TFLite for lightweight deployment\n",
    "converter = tf.lite.TFLiteConverter.from_saved_model(saved_model_dir)\n",
    "converter.optimizations = [tf.lite.Optimize.DEFAULT]\n",
    "tflite_model = converter.convert()\n",
    "\n",
    "tflite_path = 'quality_classifier.tflite'\n",
    "with open(tflite_path, 'wb') as f:\n",
    "    f.write(tflite_model)\n",
    "\n",
    "tflite_size = os.path.getsize(tflite_path) / (1024 * 1024)\n",
    "print(f\"Saved TFLite model: {tflite_path} ({tflite_size:.1f} MB)\")\n",
    "print(f\"Size reduction: {h5_size/tflite_size:.1f}x smaller than .h5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download the trained models\n",
    "from google.colab import files\n",
    "\n",
    "print(\"Downloading models...\")\n",
    "files.download(h5_path)\n",
    "files.download(tflite_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 9. Quick Inference Test\n",
    "\n",
    "Use this cell to test the model on individual images after training."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "def predict_quality(image_path, model, img_size=224):\n    \"\"\"Predict whether a single photo is good, bad, or unreadable.\"\"\"\n    img = keras.utils.load_img(image_path, target_size=(img_size, img_size))\n    img_array = keras.utils.img_to_array(img)\n    img_array = np.expand_dims(img_array, axis=0)\n\n    predictions = model.predict(img_array, verbose=0)[0]\n    pred_idx = np.argmax(predictions)\n    label = class_names[pred_idx]\n    confidence = predictions[pred_idx]\n\n    return label, float(confidence)\n\n# Example: test on a few validation images\nimport random\ntest_dir = os.path.join(DATA_DIR, 'good')\ntest_files = random.sample(os.listdir(test_dir), min(3, len(os.listdir(test_dir))))\nfor fname in test_files:\n    fpath = os.path.join(test_dir, fname)\n    label, conf = predict_quality(fpath, model)\n    print(f\"{fname}: {label} ({conf:.1%} confidence)\")"
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "---\n\n## Next Steps\n\n1. Download the `.h5` and `.tflite` models\n2. Place them in the project for Stage 2 integration\n3. If accuracy < 80%, try:\n   - Adding more labeled data (especially for the weaker class)\n   - Increasing augmentation intensity\n   - Training for more epochs\n4. **Model outputs for each photo:**\n   - **good** → accept the photo\n   - **bad** → prompt tech to retake\n   - **unreadable** → flag for manual entry (label is physically damaged)"
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}